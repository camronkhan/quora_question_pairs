{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\camro\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../word2vec.gi/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does it include the stop words like 'a', 'and', 'the'? 0 0 1\n"
     ]
    }
   ],
   "source": [
    "# Does the model include stop words?\n",
    "print(\"Does it include the stop words like \\'a\\', \\'and\\', \\'the\\'? %d %d %d\" % ('a' in model.vocab, 'and' in model.vocab, 'the' in model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the entire list of \"words\" from the Google Word2Vec model, and write\n",
    "# these out to text files so we can peruse them.\n",
    "vocab = list(model.vocab.keys())\n",
    "fileNum = 1\n",
    "wordsInVocab = len(vocab)\n",
    "wordsPerFile = int(100E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsPerFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out the words in 100k chunks.\n",
    "for wordIndex in range(0, wordsInVocab, wordsPerFile):\n",
    "    # Write out the chunk to a numbered text file.    \n",
    "    with open('./vocab.gi/vocab_%.2d.txt' % fileNum, 'w') as f:\n",
    "        # For each word in the current chunk...        \n",
    "        for i in range(wordIndex, wordIndex + wordsPerFile):\n",
    "            # Write it out and escape any unicode characters.            \n",
    "            f.write(str(vocab[i].encode('UTF-8')) + '\\n')\n",
    "    fileNum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
