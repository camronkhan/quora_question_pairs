{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gensim.models.doc2vec as d2v\n",
    "import multiprocessing as mp\n",
    "import datetime as dt\n",
    "\n",
    "from collections import OrderedDict\n",
    "from random import shuffle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cores = mp.cpu_count() - 2\n",
    "assert d2v.FAST_VERSION > -1, \"Doc2Vec will run painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import training data into dataframe\n",
    "TRAIN = pd.read_csv('../data.gi/train.csv')\n",
    "TEST = pd.read_csv('../data.gi/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create lookup table, then pickle\n",
    "train_lookup_df = TRAIN[['id', 'qid1', 'qid2', 'is_duplicate']]\n",
    "train_lookup_df.to_pickle('./pickles.gi/train_lookup_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then stack, sort, and reindex new dataframe\n",
    "train_q1_df = TRAIN[['id', 'qid1', 'question1']]\n",
    "train_q1_df.columns = ['pid', 'qid', 'question']\n",
    "train_q2_df = TRAIN[['id', 'qid2', 'question2']]\n",
    "train_q2_df.columns = ['pid', 'qid', 'question']\n",
    "train_questions_df = pd.concat([train_q1_df, train_q2_df], ignore_index=True).sort_values(by=['pid', 'qid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add test set indicator\n",
    "values = [0] * len(train_questions_df.index)\n",
    "train_questions_df = train_questions_df.assign(test=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add qid's for question1 and question2\n",
    "odd_range = pd.Series(range(1, len(TEST.index) * 2 + 1, 2))\n",
    "even_range = pd.Series(range(2, len(TEST.index) * 2 + 1, 2))\n",
    "TEST = TEST.assign(qid1=odd_range, qid2=even_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate q1 and q2 into respective dataframes\n",
    "# Then stack, sort, and reindex new dataframe\n",
    "test_q1_df = TEST[['test_id', 'qid1', 'question1']]\n",
    "test_q1_df.columns = ['pid', 'qid', 'question']\n",
    "test_q2_df = TEST[['test_id', 'qid2', 'question2']]\n",
    "test_q2_df.columns = ['pid', 'qid', 'question']\n",
    "test_questions_df = pd.concat([test_q1_df, test_q2_df], ignore_index=True).sort_values(by=['pid', 'qid']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add test set flag\n",
    "values = [1] * len(test_questions_df.index)\n",
    "test_questions_df = test_questions_df.assign(test=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine train and test sets\n",
    "# Move test flag column to first position\n",
    "combined_questions_df = pd.concat([train_questions_df, test_questions_df], ignore_index=True).sort_values(by=['test', 'pid', 'qid']).reset_index(drop=True)\n",
    "cols = combined_questions_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "combined_questions_df = combined_questions_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse to string, force lowercasing, tokenize, filter out stopwords, and stem\n",
    "def preprocessText(questions):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stopwords = set('for a of the and to in'.split(' '))\n",
    "    lowered = [str(question).lower() for question in questions]\n",
    "    tokenized = [tokenizer.tokenize(question) for question in lowered]\n",
    "    filtered = [[token for token in tokens if token not in stopwords] for tokens in tokenized]\n",
    "    stemmed = [[stemmer.stem(token) for token in tokens if token not in stopwords] for tokens in filtered]\n",
    "    return stemmed\n",
    "\n",
    "# TODO: Research lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess and pickle\n",
    "all_questions_df = combined_questions_df.assign(tokens=preprocessText(combined_questions_df['question']))\n",
    "all_questions_df.to_pickle('./pickles.gi/all_questions_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all_questions_df = pd.read_pickle('./pickles.gi/all_questions_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get tagged documents\n",
    "tagged_docs = [d2v.TaggedDocument(row[5], [row[1], row[3]]) for row in all_questions_df.itertuples()]\n",
    "train_docs = [doc for doc in tagged_docs if doc[1][0] == 0]\n",
    "test_docs = [doc for doc in tagged_docs if doc[1][0] == 1]\n",
    "doc_list = tagged_docs[:]  # for reshuffling per pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Build Doc2Vec Model Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-04-25 13:11:55.605340\n",
      "END 2017-04-25 13:14:58.076102\n"
     ]
    }
   ],
   "source": [
    "print(\"START %s\" % dt.datetime.now())\n",
    "\n",
    "dmc_model = d2v.Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "dmc_model.build_vocab(tagged_docs)\n",
    "dmc_model.save('./models.gi/dmc_model.build_vocab')\n",
    "\n",
    "print(\"END %s\" % str(dt.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-04-25 13:18:44.512104\n",
      "END 2017-04-25 13:24:38.579115\n"
     ]
    }
   ],
   "source": [
    "print(\"START %s\" % dt.datetime.now())\n",
    "\n",
    "dbow_model = d2v.Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores)\n",
    "dbow_model.build_vocab(tagged_docs)\n",
    "dbow_model.save('./models.gi/dbow_model.build_vocab')\n",
    "\n",
    "print(\"END %s\" % str(dt.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-04-25 13:30:00.605678\n",
      "END 2017-04-25 13:43:42.225713\n"
     ]
    }
   ],
   "source": [
    "print(\"START %s\" % dt.datetime.now())\n",
    "\n",
    "dmm_model = d2v.Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores)\n",
    "dmm_model.build_vocab(tagged_docs)\n",
    "dmm_model.save('./models.gi/dmm_model.build_vocab')\n",
    "\n",
    "print(\"END %s\" % str(dt.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
